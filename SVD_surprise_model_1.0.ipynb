{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2XkpwKLv9lr"
      },
      "outputs": [],
      "source": [
        "# first try of recommendation system model creation\n",
        "\n",
        "\n",
        "\"\"\"imports\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from logger import logger\n",
        "import os\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import models, layers, utils, Model, optimizers, activations, callbacks, losses, metrics\n",
        "from keras import backend as K\n",
        "\n",
        "# allow to display ALL columns from dataframe\n",
        "pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.max_rows', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load\"\"\"\n",
        "\n",
        "# base path to data\n",
        "base_path = r\"F:/large_data/BX-CSV-Dump/\"\n",
        "\n",
        "# load all files\n",
        "rating_data = pd.read_csv(\n",
        "    base_path+\"BX-Book-Ratings.csv\", sep=\";\",\n",
        ")\n",
        "user_data = pd.read_csv(\n",
        "    base_path+\"BX-Users.csv\", sep=\";\",\n",
        ")\n",
        "books_data = pd.read_csv(\n",
        "    base_path+\"BX-Books.csv\", sep=r'\";\"', low_memory=True, engine=\"python\",\n",
        "    usecols=['\"ISBN', \"Book-Title\", \"Book-Author\", \"Year-Of-Publication\", \"Publisher\"],\n",
        ")\n",
        "\n",
        "# show some data\n",
        "# print(rating_data)\n",
        "# print(rating_data.describe())\n",
        "# print(user_data)\n",
        "# print(user_data.describe())\n",
        "# print(books_data)\n",
        "# print(books_data.describe())\n",
        "\n",
        "\n",
        "\"\"\"pre-processing of data\"\"\"\n",
        "\n",
        "\n",
        "# merge\n",
        "# rating_data = rating_data.merge(books_data, how=\"left\", left_on=\"ISBN\", right_index=True)\n",
        "# print(rating_data)\n",
        "\n",
        "\n",
        "\"\"\"books_data processing\"\"\"\n",
        "\n",
        "# clear \"Year-Of-Publication\" column\n",
        "books_data[\"Year-Of-Publication\"] = books_data[\"Year-Of-Publication\"].fillna(0)\n",
        "books_data[\"Year-Of-Publication\"] = books_data[\"Year-Of-Publication\"]\\\n",
        "    .apply(lambda x: int(x) if len(re.findall(r\"[^a-z]\", f\"{x}\".lower())) == 4 else 0).astype(\"int64\")\n",
        "\n",
        "\n",
        "# rename \"ISBN column to ISBN\n",
        "books_data = books_data.rename(columns={'\"ISBN': 'ISBN'})\n",
        "\n",
        "# delete excess first symbol\n",
        "books_data[\"ISBN\"] = books_data[\"ISBN\"].apply(lambda x: str(x)[1:])\n",
        "\n",
        "# add indexes as books numbers column\n",
        "books_data[\"ISBN-Encoded\"] = books_data.index.astype(\"int64\")\n",
        "\n",
        "\n",
        "# fill authors NaNs\n",
        "books_data[\"Book-Author\"] = books_data[\"Book-Author\"].fillna(\"None\")\n",
        "\n",
        "# create dict and encode all book authors to numbers\n",
        "unique_authors = books_data[\"Book-Author\"].unique()\n",
        "author_to_number = {author: number for number, author in enumerate(unique_authors)}\n",
        "number_to_author = {number: author for number, author in enumerate(unique_authors)}\n",
        "del unique_authors\n",
        "\n",
        "# add encoded authors as column\n",
        "books_data[\"Book-Author-Encoded\"] = books_data[\"Book-Author\"].apply(lambda x: int(author_to_number[x])).astype(\"int64\")\n",
        "# print(books_data[\"Book-Author-Encoded\"])\n",
        "\n",
        "\n",
        "# fill Publisher NaNs\n",
        "books_data[\"Publisher\"] = books_data[\"Publisher\"].fillna(\"None\")\n",
        "\n",
        "# create dict and encode all book Publishers to numbers\n",
        "unique_publisher = books_data[\"Publisher\"].unique()\n",
        "publisher_to_number = {publisher: number for number, publisher in enumerate(unique_publisher)}\n",
        "number_to_publisher = {number: publisher for number, publisher in enumerate(unique_publisher)}\n",
        "\n",
        "# add encoded publishers as column\n",
        "books_data[\"Publisher-Encoded\"] = books_data[\"Publisher\"].apply(lambda x: int(publisher_to_number[x])).astype(\"int64\")\n",
        "# print(books_data[\"Publisher-Encoded\"])\n",
        "\n",
        "\n",
        "# fill Book-Title NaNs\n",
        "books_data[\"Book-Title\"] = books_data[\"Book-Title\"].fillna(\"None\")\n",
        "\n",
        "# process words in titles\n",
        "books_data[\"Book-Title-Encoded\"] = books_data[\"Book-Title\"].apply(lambda x: re.sub(r\"[^a-z ]\", \"\", x.lower()).strip())\n",
        "\n",
        "# create dicts for symbols encoding\n",
        "symbol_to_number = {symbol: number+1 for number, symbol in enumerate(string.ascii_lowercase+\" \")}\n",
        "number_to_symbol = {number+1: symbol for number, symbol in enumerate(string.ascii_lowercase+\" \")}\n",
        "\n",
        "# encode all titles on chars level\n",
        "books_data[\"Book-Title-Encoded\"] = books_data[\"Book-Title-Encoded\"].apply(lambda x: list(symbol_to_number[char]\n",
        "                                                                                         for char in str(x)))\n",
        "# extend them to one len\n",
        "books_data[\"Book-Title-Encoded\"] = list(utils.pad_sequences(sequences=books_data[\"Book-Title-Encoded\"], value=0))\n",
        "# print(books_data[\"Book-Title-Encoded\"])\n",
        "\n",
        "\n",
        "# show all processed books_data in one\n",
        "# print(books_data[[\"Year-Of-Publication\", \"ISBN-Encoded\", \"Book-Author-Encoded\", \"Publisher-Encoded\", \"Book-Title-Encoded\"]])\n",
        "# print(books_data[[\"Year-Of-Publication\", \"ISBN-Encoded\", \"Book-Author-Encoded\", \"Publisher-Encoded\", \"Book-Title-Encoded\"]].describe())\n",
        "\n",
        "\n",
        "\"\"\"user_data processing\"\"\"\n",
        "\n",
        "# fill NaNs in Age column with 0\n",
        "user_data[\"Age\"] = user_data[\"Age\"].fillna(0)\n",
        "\n",
        "# standardize and scale age in renge (0., 1.)\n",
        "user_data[\"Age-Encoded\"] = preprocessing.MinMaxScaler().fit_transform(np.asarray(user_data[\"Age\"]).reshape(-1, 1)).reshape(-1,)\n",
        "# print(user_data[\"Age-Encoded\"])\n",
        "\n",
        "\n",
        "# fill NaNs in Location column\n",
        "user_data[\"Location\"] = user_data[\"Location\"].fillna(\"n/a\")\n",
        "\n",
        "# split location string by \", \" and reshape to (-1,)\n",
        "user_data[\"Location-Encoded\"] = user_data[\"Location\"].apply(lambda x: re.split(\", \", x.lower().strip()))\n",
        "# flatten data to 1-d array\n",
        "user_data_unique_locations = np.asarray([element for sublist in user_data[\"Location-Encoded\"] for element in sublist])\n",
        "# take all unique locations\n",
        "user_data_unique_locations = np.unique(user_data_unique_locations.reshape(-1,).astype(\"str\"), axis=-1)\n",
        "# add blank value to list as first element\n",
        "user_data_unique_locations = np.append(user_data_unique_locations, [\"blank_value\"], axis=-1)[::-1]\n",
        "\n",
        "# create dicts for users locations\n",
        "location_to_number = {location: number for number, location in enumerate(user_data_unique_locations)}\n",
        "number_to_location = {number: location for number, location in enumerate(user_data_unique_locations)}\n",
        "# del user_data_unique_locations\n",
        "\n",
        "# encode all users locations\n",
        "user_data[\"Location-Encoded\"] = user_data[\"Location-Encoded\"].apply(lambda x: list(location_to_number[element]\n",
        "                                                                                   for element in x))\n",
        "# extend all location sequences to one len\n",
        "user_data[\"Location-Encoded\"] = list(utils.pad_sequences(sequences=user_data[\"Location-Encoded\"],\n",
        "                                                         value=location_to_number[\"blank_value\"]))\n",
        "\n",
        "\n",
        "# show all processed user_data in one\n",
        "# print(user_data[[\"Age-Encoded\", \"Location-Encoded\"]])\n",
        "# print(user_data[[\"Age-Encoded\", \"Location-Encoded\"]].describe())\n",
        "\n",
        "\n",
        "\"\"\"rating_data processing and merging it all together\"\"\"\n",
        "\n",
        "# standardize and scale books ratings in range (0.5, 1)\n",
        "rating_data[\"Book-Rating-Encoded\"] = preprocessing.MinMaxScaler(feature_range=(0., 1.))\\\n",
        "    .fit_transform(np.asarray(rating_data[\"Book-Rating\"]).reshape(-1, 1)).reshape(-1,)\n",
        "# print(rating_data[\"Book-Rating-Encoded\"])\n",
        "\n",
        "\n",
        "# re-give type to all columns\n",
        "rating_data[\"ISBN\"], books_data[\"ISBN\"] = rating_data[\"ISBN\"].astype(\"str\"), books_data[\"ISBN\"].astype(\"str\")\n",
        "rating_data[\"User-ID\"], rating_data[\"Book-Rating\"] = rating_data[\"User-ID\"].astype(\"int64\"), rating_data[\"Book-Rating\"].astype(\"float64\")\n",
        "rating_data[\"Book-Rating-Encoded\"] = rating_data[\"Book-Rating-Encoded\"].astype(\"float64\")\n",
        "user_data[\"User-ID\"], user_data[\"Location\"] = user_data[\"User-ID\"].astype(\"int64\"), user_data[\"Location\"].astype(\"str\")\n",
        "user_data[\"Age\"], user_data[\"Age-Encoded\"] = user_data[\"Age\"].astype(\"float64\"), user_data[\"Age-Encoded\"].astype(\"float64\")\n",
        "user_data[\"Location-Encoded\"] = user_data[\"Location-Encoded\"].astype(\"object\")\n",
        "books_data[\"Book-Title\"], books_data[\"Book-Author\"] = books_data[\"Book-Title\"].astype(\"str\"), books_data[\"Book-Author\"].astype(\"str\")\n",
        "books_data[\"Publisher\"] = books_data[\"Publisher\"].astype(\"str\")\n",
        "\n",
        "# merge all dataframes to one by rating_data\n",
        "rating_data = pd.merge(left=rating_data, right=user_data, how=\"left\", on=\"User-ID\")\n",
        "rating_data = pd.merge(left=rating_data, right=books_data, how=\"left\", on=\"ISBN\")\n",
        "\n",
        "# delete irrelevant dataframes\n",
        "del user_data, books_data\n",
        "\n",
        "# delete all rows with NaNs\n",
        "rating_data = rating_data.drop(rating_data[rating_data.isnull().any(axis=1)].index)\n",
        "\n"
      ],
      "metadata": {
        "id": "9lyk07A7wHL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"final data\"\"\"\n",
        "\n",
        "# take only rows with rating >= 0.1\n",
        "rating_data = rating_data.loc[rating_data['Book-Rating-Encoded'] >= 0.1]\n",
        "\n",
        "# take only rows where products have >= 20 reviews\n",
        "rating_data = rating_data.groupby(\"ISBN-Encoded\").filter(lambda x: len(x) >= 20)\n",
        "# # take only rows with users who make >= 20 reviews\n",
        "rating_data = rating_data.groupby(\"User-ID\").filter(lambda x: len(x) >= 20)\n",
        "\n",
        "# # take part of data with only threshold_value users and threshold_value books\n",
        "# threshold_value = 50000\n",
        "# rating_data = rating_data.loc[rating_data['ISBN-Encoded'] <= threshold_value]\n",
        "# rating_data = rating_data.loc[rating_data['User-ID'] <= threshold_value]\n",
        "\n",
        "\n",
        "# shuffle all rows in dataframe\n",
        "rating_data = rating_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# show final full dataframe\n",
        "print(rating_data)\n",
        "print(rating_data.describe())\n",
        "\n",
        "# # take and show only useful for work of model data\n",
        "# useful_data = rating_data[[\"User-ID\", \"ISBN-Encoded\", \"Book-Rating-Encoded\", \"Age-Encoded\", \"Location-Encoded\",\n",
        "#                            \"Year-Of-Publication\", \"Book-Author-Encoded\", \"Publisher-Encoded\", \"Book-Title-Encoded\"]]\n",
        "# print(useful_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UXiKcWNuwSrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"surprise SVD model\"\"\"\n",
        "\n",
        "# prepare train and test sets\n",
        "reader = Reader(rating_scale=(0., 1.))\n",
        "data = Dataset.load_from_df(rating_data[[\"User-ID\", \"ISBN-Encoded\", \"Book-Rating-Encoded\"]], reader)\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# init and train the funk mf model\n",
        "algo = SVD(n_factors=100, n_epochs=20)\n",
        "algo.fit(train)\n",
        "pred = algo.test(test)\n",
        "\n",
        "# evaluation the test set\n",
        "model_accuracy = accuracy.rmse(pred)\n",
        "print(f\"model accuracy: {model_accuracy}\")\n",
        "\n",
        "# compare of real and predicted rating of test dataset\n",
        "for c1, prediction in enumerate(pred):\n",
        "    if c1 < 100:\n",
        "        print(f\"prediction {prediction}\")\n",
        "        print(f\"real {test[c1]}\")\n"
      ],
      "metadata": {
        "id": "2_KzSfm-wHCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHpdyFDXwG_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-nVckKJwG9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onO-75APwG65"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
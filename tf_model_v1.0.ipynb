{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2XkpwKLv9lr"
      },
      "outputs": [],
      "source": [
        "# first try of recommendation system model creation\n",
        "\n",
        "\n",
        "\"\"\"imports\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from logger import logger\n",
        "import os\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import models, layers, utils, Model, optimizers, activations, callbacks, losses, metrics\n",
        "from keras import backend as K\n",
        "\n",
        "# allow to display ALL columns from dataframe\n",
        "pd.set_option('display.max_columns', None)\n",
        "# pd.set_option('display.max_rows', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load\"\"\"\n",
        "\n",
        "# base path to data\n",
        "base_path = r\"F:/large_data/BX-CSV-Dump/\"\n",
        "\n",
        "# load all files\n",
        "rating_data = pd.read_csv(\n",
        "    base_path+\"BX-Book-Ratings.csv\", sep=\";\",\n",
        ")\n",
        "user_data = pd.read_csv(\n",
        "    base_path+\"BX-Users.csv\", sep=\";\",\n",
        ")\n",
        "books_data = pd.read_csv(\n",
        "    base_path+\"BX-Books.csv\", sep=r'\";\"', low_memory=True, engine=\"python\",\n",
        "    usecols=['\"ISBN', \"Book-Title\", \"Book-Author\", \"Year-Of-Publication\", \"Publisher\"],\n",
        ")\n",
        "\n",
        "# show some data\n",
        "# print(rating_data)\n",
        "# print(rating_data.describe())\n",
        "# print(user_data)\n",
        "# print(user_data.describe())\n",
        "# print(books_data)\n",
        "# print(books_data.describe())\n",
        "\n",
        "\n",
        "\"\"\"pre-processing of data\"\"\"\n",
        "\n",
        "\n",
        "# merge\n",
        "# rating_data = rating_data.merge(books_data, how=\"left\", left_on=\"ISBN\", right_index=True)\n",
        "# print(rating_data)\n",
        "\n",
        "\n",
        "\"\"\"books_data processing\"\"\"\n",
        "\n",
        "# clear \"Year-Of-Publication\" column\n",
        "books_data[\"Year-Of-Publication\"] = books_data[\"Year-Of-Publication\"].fillna(0)\n",
        "books_data[\"Year-Of-Publication\"] = books_data[\"Year-Of-Publication\"]\\\n",
        "    .apply(lambda x: int(x) if len(re.findall(r\"[^a-z]\", f\"{x}\".lower())) == 4 else 0).astype(\"int64\")\n",
        "\n",
        "\n",
        "# rename \"ISBN column to ISBN\n",
        "books_data = books_data.rename(columns={'\"ISBN': 'ISBN'})\n",
        "\n",
        "# delete excess first symbol\n",
        "books_data[\"ISBN\"] = books_data[\"ISBN\"].apply(lambda x: str(x)[1:])\n",
        "\n",
        "# add indexes as books numbers column\n",
        "books_data[\"ISBN-Encoded\"] = books_data.index.astype(\"int64\")\n",
        "\n",
        "\n",
        "# fill authors NaNs\n",
        "books_data[\"Book-Author\"] = books_data[\"Book-Author\"].fillna(\"None\")\n",
        "\n",
        "# create dict and encode all book authors to numbers\n",
        "unique_authors = books_data[\"Book-Author\"].unique()\n",
        "author_to_number = {author: number for number, author in enumerate(unique_authors)}\n",
        "number_to_author = {number: author for number, author in enumerate(unique_authors)}\n",
        "del unique_authors\n",
        "\n",
        "# add encoded authors as column\n",
        "books_data[\"Book-Author-Encoded\"] = books_data[\"Book-Author\"].apply(lambda x: int(author_to_number[x])).astype(\"int64\")\n",
        "# print(books_data[\"Book-Author-Encoded\"])\n",
        "\n",
        "\n",
        "# fill Publisher NaNs\n",
        "books_data[\"Publisher\"] = books_data[\"Publisher\"].fillna(\"None\")\n",
        "\n",
        "# create dict and encode all book Publishers to numbers\n",
        "unique_publisher = books_data[\"Publisher\"].unique()\n",
        "publisher_to_number = {publisher: number for number, publisher in enumerate(unique_publisher)}\n",
        "number_to_publisher = {number: publisher for number, publisher in enumerate(unique_publisher)}\n",
        "\n",
        "# add encoded publishers as column\n",
        "books_data[\"Publisher-Encoded\"] = books_data[\"Publisher\"].apply(lambda x: int(publisher_to_number[x])).astype(\"int64\")\n",
        "# print(books_data[\"Publisher-Encoded\"])\n",
        "\n",
        "\n",
        "# fill Book-Title NaNs\n",
        "books_data[\"Book-Title\"] = books_data[\"Book-Title\"].fillna(\"None\")\n",
        "\n",
        "# process words in titles\n",
        "books_data[\"Book-Title-Encoded\"] = books_data[\"Book-Title\"].apply(lambda x: re.sub(r\"[^a-z ]\", \"\", x.lower()).strip())\n",
        "\n",
        "# create dicts for symbols encoding\n",
        "symbol_to_number = {symbol: number+1 for number, symbol in enumerate(string.ascii_lowercase+\" \")}\n",
        "number_to_symbol = {number+1: symbol for number, symbol in enumerate(string.ascii_lowercase+\" \")}\n",
        "\n",
        "# encode all titles on chars level\n",
        "books_data[\"Book-Title-Encoded\"] = books_data[\"Book-Title-Encoded\"].apply(lambda x: list(symbol_to_number[char]\n",
        "                                                                                         for char in str(x)))\n",
        "# extend them to one len\n",
        "books_data[\"Book-Title-Encoded\"] = list(utils.pad_sequences(sequences=books_data[\"Book-Title-Encoded\"], value=0))\n",
        "# print(books_data[\"Book-Title-Encoded\"])\n",
        "\n",
        "\n",
        "# show all processed books_data in one\n",
        "# print(books_data[[\"Year-Of-Publication\", \"ISBN-Encoded\", \"Book-Author-Encoded\", \"Publisher-Encoded\", \"Book-Title-Encoded\"]])\n",
        "# print(books_data[[\"Year-Of-Publication\", \"ISBN-Encoded\", \"Book-Author-Encoded\", \"Publisher-Encoded\", \"Book-Title-Encoded\"]].describe())\n",
        "\n",
        "\n",
        "\"\"\"user_data processing\"\"\"\n",
        "\n",
        "# fill NaNs in Age column with 0\n",
        "user_data[\"Age\"] = user_data[\"Age\"].fillna(0)\n",
        "\n",
        "# standardize and scale age in renge (0., 1.)\n",
        "user_data[\"Age-Encoded\"] = preprocessing.MinMaxScaler().fit_transform(np.asarray(user_data[\"Age\"]).reshape(-1, 1)).reshape(-1,)\n",
        "# print(user_data[\"Age-Encoded\"])\n",
        "\n",
        "\n",
        "# fill NaNs in Location column\n",
        "user_data[\"Location\"] = user_data[\"Location\"].fillna(\"n/a\")\n",
        "\n",
        "# split location string by \", \" and reshape to (-1,)\n",
        "user_data[\"Location-Encoded\"] = user_data[\"Location\"].apply(lambda x: re.split(\", \", x.lower().strip()))\n",
        "# flatten data to 1-d array\n",
        "user_data_unique_locations = np.asarray([element for sublist in user_data[\"Location-Encoded\"] for element in sublist])\n",
        "# take all unique locations\n",
        "user_data_unique_locations = np.unique(user_data_unique_locations.reshape(-1,).astype(\"str\"), axis=-1)\n",
        "# add blank value to list as first element\n",
        "user_data_unique_locations = np.append(user_data_unique_locations, [\"blank_value\"], axis=-1)[::-1]\n",
        "\n",
        "# create dicts for users locations\n",
        "location_to_number = {location: number for number, location in enumerate(user_data_unique_locations)}\n",
        "number_to_location = {number: location for number, location in enumerate(user_data_unique_locations)}\n",
        "# del user_data_unique_locations\n",
        "\n",
        "# encode all users locations\n",
        "user_data[\"Location-Encoded\"] = user_data[\"Location-Encoded\"].apply(lambda x: list(location_to_number[element]\n",
        "                                                                                   for element in x))\n",
        "# extend all location sequences to one len\n",
        "user_data[\"Location-Encoded\"] = list(utils.pad_sequences(sequences=user_data[\"Location-Encoded\"],\n",
        "                                                         value=location_to_number[\"blank_value\"]))\n",
        "\n",
        "\n",
        "# show all processed user_data in one\n",
        "# print(user_data[[\"Age-Encoded\", \"Location-Encoded\"]])\n",
        "# print(user_data[[\"Age-Encoded\", \"Location-Encoded\"]].describe())\n",
        "\n",
        "\n",
        "\"\"\"rating_data processing and merging it all together\"\"\"\n",
        "\n",
        "# standardize and scale books ratings in range (0.5, 1)\n",
        "rating_data[\"Book-Rating-Encoded\"] = preprocessing.MinMaxScaler(feature_range=(0., 1.))\\\n",
        "    .fit_transform(np.asarray(rating_data[\"Book-Rating\"]).reshape(-1, 1)).reshape(-1,)\n",
        "# print(rating_data[\"Book-Rating-Encoded\"])\n",
        "\n",
        "\n",
        "# re-give type to all columns\n",
        "rating_data[\"ISBN\"], books_data[\"ISBN\"] = rating_data[\"ISBN\"].astype(\"str\"), books_data[\"ISBN\"].astype(\"str\")\n",
        "rating_data[\"User-ID\"], rating_data[\"Book-Rating\"] = rating_data[\"User-ID\"].astype(\"int64\"), rating_data[\"Book-Rating\"].astype(\"float64\")\n",
        "rating_data[\"Book-Rating-Encoded\"] = rating_data[\"Book-Rating-Encoded\"].astype(\"float64\")\n",
        "user_data[\"User-ID\"], user_data[\"Location\"] = user_data[\"User-ID\"].astype(\"int64\"), user_data[\"Location\"].astype(\"str\")\n",
        "user_data[\"Age\"], user_data[\"Age-Encoded\"] = user_data[\"Age\"].astype(\"float64\"), user_data[\"Age-Encoded\"].astype(\"float64\")\n",
        "user_data[\"Location-Encoded\"] = user_data[\"Location-Encoded\"].astype(\"object\")\n",
        "books_data[\"Book-Title\"], books_data[\"Book-Author\"] = books_data[\"Book-Title\"].astype(\"str\"), books_data[\"Book-Author\"].astype(\"str\")\n",
        "books_data[\"Publisher\"] = books_data[\"Publisher\"].astype(\"str\")\n",
        "\n",
        "# merge all dataframes to one by rating_data\n",
        "rating_data = pd.merge(left=rating_data, right=user_data, how=\"left\", on=\"User-ID\")\n",
        "rating_data = pd.merge(left=rating_data, right=books_data, how=\"left\", on=\"ISBN\")\n",
        "\n",
        "# delete irrelevant dataframes\n",
        "del user_data, books_data\n",
        "\n",
        "# delete all rows with NaNs\n",
        "rating_data = rating_data.drop(rating_data[rating_data.isnull().any(axis=1)].index)\n",
        "\n"
      ],
      "metadata": {
        "id": "9lyk07A7wHL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"final data\"\"\"\n",
        "\n",
        "# take only rows with rating >= 0.1\n",
        "rating_data = rating_data.loc[rating_data['Book-Rating-Encoded'] >= 0.1]\n",
        "\n",
        "# take only rows where products have >= 20 reviews\n",
        "rating_data = rating_data.groupby(\"ISBN-Encoded\").filter(lambda x: len(x) >= 20)\n",
        "# # take only rows with users who make >= 20 reviews\n",
        "rating_data = rating_data.groupby(\"User-ID\").filter(lambda x: len(x) >= 20)\n",
        "\n",
        "# # take part of data with only threshold_value users and threshold_value books\n",
        "# threshold_value = 50000\n",
        "# rating_data = rating_data.loc[rating_data['ISBN-Encoded'] <= threshold_value]\n",
        "# rating_data = rating_data.loc[rating_data['User-ID'] <= threshold_value]\n",
        "\n",
        "\n",
        "# shuffle all rows in dataframe\n",
        "rating_data = rating_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# show final full dataframe\n",
        "print(rating_data)\n",
        "print(rating_data.describe())\n",
        "\n",
        "# # take and show only useful for work of model data\n",
        "# useful_data = rating_data[[\"User-ID\", \"ISBN-Encoded\", \"Book-Rating-Encoded\", \"Age-Encoded\", \"Location-Encoded\",\n",
        "#                            \"Year-Of-Publication\", \"Book-Author-Encoded\", \"Publisher-Encoded\", \"Book-Title-Encoded\"]]\n",
        "# print(useful_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UXiKcWNuwSrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"collaborative filtering recommendation system model creation\"\"\"\n",
        "\n",
        "\n",
        "# # # model params\n",
        "\n",
        "# # invariable model params from data\n",
        "# count of books in dataset\n",
        "product_count = len(rating_data[\"ISBN-Encoded\"].unique()) + 1\n",
        "# count of users\n",
        "users_count = len(rating_data[\"User-ID\"].unique()) + 1\n",
        "# user location shape\n",
        "location_shape = np.asarray(rating_data[\"Location-Encoded\"].iloc[1]).shape\n",
        "# number of unique location\n",
        "location_count = len(location_to_number) + 1\n",
        "# number of unique books publication years\n",
        "years_count = len(rating_data[\"Year-Of-Publication\"].unique()) + 1\n",
        "# books authors count\n",
        "author_count = len(rating_data[\"Book-Author-Encoded\"].unique()) + 1\n",
        "# books publisher count\n",
        "publisher_count = len(rating_data[\"Publisher-Encoded\"].unique()) + 1\n",
        "# book title shape\n",
        "title_shape = np.asarray(rating_data[\"Book-Title-Encoded\"].iloc[1]).shape\n",
        "# book title unique chars count\n",
        "title_chars_count = len(symbol_to_number) + 1\n",
        "\n",
        "# # variable model params\n",
        "# age dims\n",
        "age_dims = 1\n",
        "# year embedding\n",
        "year_embedding = 2\n",
        "# user embedding\n",
        "user_embedding = 16\n",
        "# product embedding\n",
        "product_embedding = 16\n",
        "# location embedding\n",
        "location_embedding = 8\n",
        "# author embedding\n",
        "author_embedding = 4\n",
        "# publisher embedding\n",
        "publisher_embedding = 4\n",
        "# title embedding\n",
        "title_embedding = 2\n",
        "# dim of merged metadata dense\n",
        "metadata_dense_dim = 1\n",
        "# merged dim units\n",
        "final_dense_dim = 16\n",
        "\n",
        "print(f\"product_count: {product_count} ; users_count: {users_count} ; location_shape: {location_shape} ; \"\n",
        "      f\"location_count: {location_count} ; years_count: {years_count}; author_count: {author_count} ; \"\n",
        "      f\"publisher_count: {publisher_count} ; title_shape: {title_shape} ; title_chars_count: {title_chars_count}\")\n",
        "\n",
        "print(f\"age_dims: {age_dims} ; year_embedding: {year_embedding} ; user_embedding: {user_embedding} ; \"\n",
        "      f\"product_embedding: {product_embedding} ; \"\n",
        "      f\"location_embedding: {location_embedding} ; author_embedding: {author_embedding} ; \"\n",
        "      f\"publisher_embedding: {publisher_embedding} ; title_embedding: {title_embedding} ; \"\n",
        "      f\"final_dense_dim: {final_dense_dim}\")\n",
        "\n",
        "\n",
        "# # # user-product part\n",
        "\n",
        "# # user-product inputs layers\n",
        "user_id_input = layers.Input(name=\"user_id_input\", shape=(1,), dtype=\"int64\")\n",
        "product_id_input = layers.Input(name=\"product_id_input\", shape=(1,), dtype=\"int64\")\n",
        "\n",
        "# # Matrix Factorization branch\n",
        "\n",
        "# user embeddings and reshape\n",
        "mf_user_x = layers.Embedding(input_dim=users_count, output_dim=user_embedding, input_length=1,\n",
        "                                     name=\"mf_user_embedding\")(user_id_input)\n",
        "mf_user_x = layers.Reshape(target_shape=(user_embedding,),\n",
        "                                 name=\"mf_user_reshape\")(mf_user_x)\n",
        "# books embedding and reshape\n",
        "mf_product_x = layers.Embedding(input_dim=product_count, output_dim=product_embedding, input_length=1,\n",
        "                                        name=\"mf_product_embedding\")(product_id_input)\n",
        "mf_product_x = layers.Reshape(target_shape=(product_embedding,),\n",
        "                                    name=\"mf_product_reshape\")(mf_product_x)\n",
        "# calculate .dot() product\n",
        "mf_output = layers.Dot(normalize=True, axes=-1, name='mf_dot_output')([mf_user_x, mf_product_x])\n",
        "\n",
        "# # Neural Network branch\n",
        "\n",
        "# user embeddings and reshape\n",
        "nn_user_x = layers.Embedding(input_dim=users_count, output_dim=user_embedding, input_length=1,\n",
        "                                     name=\"nn_user_embedding\")(user_id_input)\n",
        "nn_user_x = layers.Reshape(target_shape=(user_embedding,),\n",
        "                                 name=\"nn_user_reshape\")(nn_user_x)\n",
        "# books embedding and reshape\n",
        "nn_product_x = layers.Embedding(input_dim=product_count, output_dim=product_embedding, input_length=1,\n",
        "                                        name=\"nn_product_embedding\")(product_id_input)\n",
        "nn_product_x = layers.Reshape(target_shape=(product_embedding,),\n",
        "                                    name=\"nn_product_reshape\")(nn_product_x)\n",
        "# concatenate and dense\n",
        "nn_x = layers.Concatenate(axis=-1, name=\"nn_concatenate\")([nn_user_x, nn_product_x])\n",
        "nn_output = layers.Dense(name=\"nn_output\", units=product_embedding // 2, activation=\"selu\", kernel_initializer=\"lecun_normal\",)(nn_x)\n",
        "\n",
        "\n",
        "# # # context data part\n",
        "\n",
        "# # age branch\n",
        "\n",
        "# age input\n",
        "age_input = layers.Input(name=\"user_age_input\", shape=(1,), dtype=\"float32\")\n",
        "# age dense\n",
        "age_output = layers.Dense(units=age_dims, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"user_age_output\")(age_input)\n",
        "\n",
        "# # location branch\n",
        "\n",
        "# location input\n",
        "location_input = layers.Input(name=\"user_location_input\", shape=location_shape, dtype=\"int64\")\n",
        "# location embedding\n",
        "location_x = layers.Embedding(input_dim=location_count, output_dim=location_embedding, input_length=location_shape[0],\n",
        "                              mask_zero=True, name=\"location_embedding\")(location_input)\n",
        "# Conv1D and Dense\n",
        "location_x = layers.Conv1D(filters=location_embedding // 2, kernel_size=1, padding=\"same\", activation=\"selu\",\n",
        "                           kernel_initializer=\"lecun_normal\", name=\"user_location_Conv1D\")(location_x)\n",
        "location_x = layers.GlobalAveragePooling1D()(location_x)\n",
        "location_output = layers.Dense(units=location_embedding // 2, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"user_location_output\")(location_x)\n",
        "\n",
        "# # Year Of Publication branch\n",
        "\n",
        "# year input\n",
        "year_input = layers.Input(name=\"book_year_input\", shape=(1,), dtype=\"int64\")\n",
        "# year embedding\n",
        "year_x = layers.Embedding(input_dim=years_count, output_dim=year_embedding, input_length=1,\n",
        "                          name=\"book_year_embedding\")(year_input)\n",
        "year_x = layers.Reshape(target_shape=(year_embedding,), name=\"book_year_reshape\")(year_x)\n",
        "# year dense\n",
        "year_output = layers.Dense(units=year_embedding // 2, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"book_year_output\")(year_x)\n",
        "\n",
        "# # Book Author branch\n",
        "\n",
        "# author input\n",
        "author_input = layers.Input(name=\"book_author_input\", shape=(1,), dtype=\"int64\")\n",
        "# author embedding\n",
        "author_x = layers.Embedding(input_dim=author_count, output_dim=author_embedding, input_length=1,\n",
        "                            name=\"book_author_embedding\")(author_input)\n",
        "author_x = layers.Reshape(target_shape=(author_embedding,), name=\"book_author_reshape\")(author_x)\n",
        "# dense\n",
        "author_output = layers.Dense(name=\"book_author_output\", units=author_embedding // 2, activation=\"selu\", kernel_initializer=\"lecun_normal\",)(author_x)\n",
        "\n",
        "# # book Publisher branch\n",
        "\n",
        "# publisher input\n",
        "publisher_input = layers.Input(name=\"book_publisher_input\", shape=(1,), dtype=\"int64\")\n",
        "# publisher embedding\n",
        "publisher_x = layers.Embedding(input_dim=publisher_count, output_dim=publisher_embedding, input_length=1,\n",
        "                            name=\"book_publisher_embedding\")(publisher_input)\n",
        "publisher_x = layers.Reshape(target_shape=(publisher_embedding,), name=\"book_publisher_reshape\")(publisher_x)\n",
        "# dense\n",
        "publisher_output = layers.Dense(name=\"book_publisher_output\", units=publisher_embedding // 2,\n",
        "                                activation=\"selu\", kernel_initializer=\"lecun_normal\",)(publisher_x)\n",
        "\n",
        "# # Book Title branch\n",
        "\n",
        "# title input\n",
        "title_input = layers.Input(name=\"book_title_input\", shape=title_shape, dtype=\"int64\")\n",
        "# title embedding\n",
        "title_x = layers.Embedding(input_dim=title_chars_count, output_dim=title_embedding, input_length=title_shape[0],\n",
        "                              mask_zero=True, name=\"book_title_embedding\")(title_input)\n",
        "# Conv1D and Dense\n",
        "title_x = layers.Conv1D(filters=title_embedding // 2, kernel_size=1, padding=\"same\", activation=\"selu\",\n",
        "                        kernel_initializer=\"lecun_normal\", name=\"book_title_Conv1D\")(title_x)\n",
        "title_x = layers.GlobalAveragePooling1D()(title_x)\n",
        "title_output = layers.Dense(units=title_embedding // 2, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"book_title_output\")(title_x)\n",
        "\n",
        "\n",
        "# # # final concatenation and rating prediction part\n",
        "\n",
        "# # concatenate and process metadata\n",
        "merged_x = layers.Concatenate(axis=-1, name=\"concatenate_metadata\")(\n",
        "    [age_output, location_output, year_output, author_output, publisher_output, title_output])\n",
        "# merged_x = layers.Dense(units=metadata_dense_dim, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"merged_metadata_dense\")(merged_x)\n",
        "\n",
        "# # concatenate all data together\n",
        "merged_x = layers.Concatenate(axis=-1, name=\"concatenate_all_data\")(\n",
        "    [mf_output, nn_output, merged_x])\n",
        "print(f\"merged_x.shape: {merged_x.shape}\")\n",
        "\n",
        "# # process all data together\n",
        "merged_x = layers.Dense(units=final_dense_dim, activation=\"selu\", kernel_initializer=\"lecun_normal\", name=\"merged_data_dense\")(merged_x)\n",
        "\n",
        "# dropout\n",
        "# merged_x = layers.Dropout(0.25)(merged_x)\n",
        "\n",
        "# # rating prediction output\n",
        "rating_output = layers.Dense(units=1, activation=\"sigmoid\", name=\"model_output\")(merged_x)  # , activation=\"sigmoid\"\n",
        "\n",
        "\n",
        "# # # model building and compilation\n",
        "\n",
        "# # build\n",
        "rs_model = Model(inputs={\"user_id_input\": user_id_input, \"product_id_input\": product_id_input, \"age_input\": age_input,\n",
        "                         \"location_input\": location_input, \"year_input\": year_input, \"author_input\": author_input,\n",
        "                         \"publisher_input\": publisher_input, \"title_input\": title_input},\n",
        "                 outputs={\"rating_output\": rating_output},\n",
        "                 name=\"recommendation_system_model\")\n",
        "rs_model.summary()\n",
        "\n",
        "# # compile\n",
        "rmse = metrics.RootMeanSquaredError(name=\"rmse\")\n",
        "rs_model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss=\"mae\", metrics=[\"mape\", rmse, \"mse\"])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lZiJgHJhwHJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"model train\"\"\"\n",
        "\n",
        "# split data to train-test splits\n",
        "train_data, test_data = train_test_split(rating_data, test_size=0.2, shuffle=True)  # .iloc[:200000]\n",
        "train_data, test_data = train_data.reset_index(drop=True), test_data.reset_index(drop=True)\n",
        "test_data = test_data.iloc[:10000]\n",
        "\n",
        "# callbacks\n",
        "callbacks_list = [\n",
        "    callbacks.ModelCheckpoint(filepath=\"models/model_1.0.h5\", monitor=\"val_loss\", save_best_only=True,\n",
        "                              save_weights_only=True),\n",
        "    # callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n",
        "    # callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_delta=0.0001, min_lr=0.00000001),\n",
        "    callbacks.TensorBoard(log_dir=\"tensorboard/model_1.0_3\"),\n",
        "]\n",
        "# train process\n",
        "history = rs_model.fit(\n",
        "    x={\n",
        "        \"user_id_input\": np.asarray(train_data[\"User-ID\"]).astype(\"int64\"),\n",
        "        \"product_id_input\": np.asarray(train_data[\"ISBN-Encoded\"]).astype(\"int64\"),\n",
        "        \"age_input\": np.asarray(train_data[\"Age-Encoded\"]).astype(\"float32\"),\n",
        "        \"location_input\": np.stack(np.asarray(train_data[\"Location-Encoded\"])).astype(\"int64\"),\n",
        "        \"year_input\": np.asarray(train_data[\"Year-Of-Publication\"]).astype(\"int64\"),\n",
        "        \"author_input\": np.asarray(train_data[\"Book-Author-Encoded\"]).astype(\"int64\"),\n",
        "        \"publisher_input\": np.asarray(train_data[\"Publisher-Encoded\"]).astype(\"int64\"),\n",
        "        \"title_input\": np.stack(np.asarray(train_data[\"Book-Title-Encoded\"])).astype(\"int64\")\n",
        "    },\n",
        "    y={\n",
        "        \"rating_output\": np.asarray(train_data[\"Book-Rating-Encoded\"]).astype(\"float32\")\n",
        "    },\n",
        "    batch_size=75, epochs=20, validation_split=0.2, callbacks=callbacks_list\n",
        ")\n",
        "\n",
        "# show train history\n",
        "# title\n",
        "plt.suptitle(\"history of model training\")\n",
        "# set context\n",
        "sns.set_context(context=\"notebook\", font_scale=1.0)\n",
        "sns.set_style(style=\"darkgrid\", rc={'grid.color': '.5'})\n",
        "# plot\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['mape'])\n",
        "plt.plot(history.history['val_mape'])\n",
        "plt.plot(history.history['rmse'])\n",
        "plt.plot(history.history['val_rmse'])\n",
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "# legend\n",
        "plt.legend(['train_loss', 'validation_loss', 'train_mape', 'validation_mape', 'train_rmse', 'validation_rmse',\n",
        "            'train_mse', 'validation_mse'])\n",
        "# show\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jqieCDPTwHGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model weights\n",
        "# rs_model.load_weights(\"models/model_1.0.h5\")\n",
        "\n",
        "# model evaluation\n",
        "mae, mape, rmse, mse = rs_model.evaluate(x={\n",
        "        \"user_id_input\": np.asarray(test_data[\"User-ID\"]).astype(\"int64\"),\n",
        "        \"product_id_input\": np.asarray(test_data[\"ISBN-Encoded\"]).astype(\"int64\"),\n",
        "        \"age_input\": np.asarray(test_data[\"Age-Encoded\"]).astype(\"float32\"),\n",
        "        \"location_input\": np.stack(np.asarray(test_data[\"Location-Encoded\"])).astype(\"int64\"),\n",
        "        \"year_input\": np.asarray(test_data[\"Year-Of-Publication\"]).astype(\"int64\"),\n",
        "        \"author_input\": np.asarray(test_data[\"Book-Author-Encoded\"]).astype(\"int64\"),\n",
        "        \"publisher_input\": np.asarray(test_data[\"Publisher-Encoded\"]).astype(\"int64\"),\n",
        "        \"title_input\": np.stack(np.asarray(test_data[\"Book-Title-Encoded\"])).astype(\"int64\")\n",
        "    },\n",
        "    y={\n",
        "        \"rating_output\": np.asarray(test_data[\"Book-Rating-Encoded\"]).astype(\"float32\")\n",
        "    }\n",
        ")\n",
        "print(f\"mae: {mae} ; mape: {mape} ; rmse: {rmse} ; mse: {mse}\")\n",
        "\n",
        "\n",
        "# show predictions examples\n",
        "prediction = rs_model.predict(x={\n",
        "        \"user_id_input\": np.asarray(test_data[\"User-ID\"]).astype(\"int64\"),\n",
        "        \"product_id_input\": np.asarray(test_data[\"ISBN-Encoded\"]).astype(\"int64\"),\n",
        "        \"age_input\": np.asarray(test_data[\"Age-Encoded\"]).astype(\"float32\"),\n",
        "        \"location_input\": np.stack(np.asarray(test_data[\"Location-Encoded\"])).astype(\"int64\"),\n",
        "        \"year_input\": np.asarray(test_data[\"Year-Of-Publication\"]).astype(\"int64\"),\n",
        "        \"author_input\": np.asarray(test_data[\"Book-Author-Encoded\"]).astype(\"int64\"),\n",
        "        \"publisher_input\": np.asarray(test_data[\"Publisher-Encoded\"]).astype(\"int64\"),\n",
        "        \"title_input\": np.stack(np.asarray(test_data[\"Book-Title-Encoded\"])).astype(\"int64\")\n",
        "    }\n",
        ")\n",
        "\n",
        "for c1, prediction_e in enumerate(prediction[\"rating_output\"]):\n",
        "    print(f\"prediction: {round(float(prediction_e), 4)}\")\n",
        "    print(f\"real: {round(float(np.asarray(test_data['Book-Rating-Encoded'].iloc[c1]).astype('float32')), 4)}\")"
      ],
      "metadata": {
        "id": "8n-QCpFawHEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_KzSfm-wHCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHpdyFDXwG_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-nVckKJwG9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "onO-75APwG65"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
